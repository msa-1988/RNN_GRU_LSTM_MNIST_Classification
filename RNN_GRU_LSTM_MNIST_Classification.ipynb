{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPA7Q/T+JmJhEjpJSbX9/ow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msa-1988/RNN_GRU_LSTM_MNIST_Classification/blob/main/RNN_GRU_LSTM_MNIST_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MNIST dataset is a widely used benchmark dataset in the field of machine learning and computer vision. It consists of a large collection of 28x28 grayscale images of handwritten digits from 0 to 9, along with their corresponding labels. The dataset is divided into two main parts: a training set containing 60,000 images and a test set containing 10,000 images.\n",
        "\n",
        "The motivation behind using the MNIST dataset for classification tasks is twofold. Firstly, the dataset serves as a fundamental introduction to image classification problems, allowing researchers and practitioners to develop and evaluate their models on a relatively simple and well-understood task. Secondly, the dataset's simplicity and small size make it ideal for prototyping and benchmarking different machine learning algorithms and techniques.\n",
        "\n",
        "The classification problem associated with the MNIST dataset is to build a model that can accurately classify the handwritten digits into their respective classes (0 to 9). This problem is a classic example of multi-class classification, where the goal is to assign a single label to each input image based on its visual features.\n",
        "\n",
        "In this notebook, I developed three RNN models to solve this problem: RNN, GRU, and LSTM."
      ],
      "metadata": {
        "id": "U142aUhMnxSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step is to import the required libraries;\n",
        "Check if GPU device is available"
      ],
      "metadata": {
        "id": "roHwiaL1pQg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "lC2KS6rCkK2J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the hyperparametes"
      ],
      "metadata": {
        "id": "Em63Od9SpV12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyper-parameters\n",
        "# input_size = 784 # 28x28\n",
        "num_classes = 10\n",
        "num_epochs = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 28\n",
        "sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n"
      ],
      "metadata": {
        "id": "of0hUhc2kQBo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the MNIST data for traning and testing from the torchvision.dataset"
      ],
      "metadata": {
        "id": "8ltiRTJYpgZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n"
      ],
      "metadata": {
        "id": "Eymlk7cvkU1Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the RNN model I developed using nn.RNN module in torch"
      ],
      "metadata": {
        "id": "4roS1CtgpqA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Dp5LLi_Skay5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the GRU model I developed using nn.GRU module in torch"
      ],
      "metadata": {
        "id": "-vWpVyKAp2CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully connected neural network with one hidden layer\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(GRU, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "\n",
        "        # or:\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        # out, _ = self.rnn(x, h0)\n",
        "        # or:\n",
        "        out, _ = self.gru(x, h0)\n",
        "        # out, _ = self.lstm(x, (h0,c0))\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "pvQfFuXilkHQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the LSTM model I developed using nn.LSTM module in torch"
      ],
      "metadata": {
        "id": "-B6pig8bp5VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully connected neural network with one hidden layer\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "\n",
        "        out, _ = self.lstm(x, (h0,c0))\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ixOkYQM9lk2Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, I initialize the models, train them, and evaluate their accuracy."
      ],
      "metadata": {
        "id": "sX5xRSNHqBRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S0FhxufjPeG",
        "outputId": "18d449e2-eabb-4ee0-9df5-d7c9ecda0067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training RNN(\n",
            "  (rnn): RNN(28, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Epoch [1/10], Step [100/600], Loss: 1.0881\n",
            "Epoch [1/10], Step [200/600], Loss: 0.7272\n",
            "Epoch [1/10], Step [300/600], Loss: 0.5096\n",
            "Epoch [1/10], Step [400/600], Loss: 0.4605\n",
            "Epoch [1/10], Step [500/600], Loss: 0.2920\n",
            "Epoch [1/10], Step [600/600], Loss: 0.4853\n",
            "Epoch [2/10], Step [100/600], Loss: 0.3459\n",
            "Epoch [2/10], Step [200/600], Loss: 0.4100\n",
            "Epoch [2/10], Step [300/600], Loss: 0.3591\n",
            "Epoch [2/10], Step [400/600], Loss: 0.3898\n",
            "Epoch [2/10], Step [500/600], Loss: 0.1693\n",
            "Epoch [2/10], Step [600/600], Loss: 0.3356\n",
            "Epoch [3/10], Step [100/600], Loss: 0.1605\n",
            "Epoch [3/10], Step [200/600], Loss: 0.1760\n",
            "Epoch [3/10], Step [300/600], Loss: 0.1672\n",
            "Epoch [3/10], Step [400/600], Loss: 0.1899\n",
            "Epoch [3/10], Step [500/600], Loss: 0.1691\n",
            "Epoch [3/10], Step [600/600], Loss: 0.3268\n",
            "Epoch [4/10], Step [100/600], Loss: 0.1995\n",
            "Epoch [4/10], Step [200/600], Loss: 0.1237\n",
            "Epoch [4/10], Step [300/600], Loss: 0.2393\n",
            "Epoch [4/10], Step [400/600], Loss: 0.1530\n",
            "Epoch [4/10], Step [500/600], Loss: 0.1166\n",
            "Epoch [4/10], Step [600/600], Loss: 0.1882\n",
            "Epoch [5/10], Step [100/600], Loss: 0.1497\n",
            "Epoch [5/10], Step [200/600], Loss: 0.0526\n",
            "Epoch [5/10], Step [300/600], Loss: 0.0856\n",
            "Epoch [5/10], Step [400/600], Loss: 0.1062\n",
            "Epoch [5/10], Step [500/600], Loss: 0.1235\n",
            "Epoch [5/10], Step [600/600], Loss: 0.2316\n",
            "Epoch [6/10], Step [100/600], Loss: 0.3322\n",
            "Epoch [6/10], Step [200/600], Loss: 0.0974\n",
            "Epoch [6/10], Step [300/600], Loss: 0.2763\n",
            "Epoch [6/10], Step [400/600], Loss: 0.0863\n",
            "Epoch [6/10], Step [500/600], Loss: 0.0551\n",
            "Epoch [6/10], Step [600/600], Loss: 0.1774\n",
            "Epoch [7/10], Step [100/600], Loss: 0.0735\n",
            "Epoch [7/10], Step [200/600], Loss: 0.1053\n",
            "Epoch [7/10], Step [300/600], Loss: 0.0869\n",
            "Epoch [7/10], Step [400/600], Loss: 0.1808\n",
            "Epoch [7/10], Step [500/600], Loss: 0.1936\n",
            "Epoch [7/10], Step [600/600], Loss: 0.0711\n",
            "Epoch [8/10], Step [100/600], Loss: 0.0586\n",
            "Epoch [8/10], Step [200/600], Loss: 0.1350\n",
            "Epoch [8/10], Step [300/600], Loss: 0.1842\n",
            "Epoch [8/10], Step [400/600], Loss: 0.0763\n",
            "Epoch [8/10], Step [500/600], Loss: 0.1132\n",
            "Epoch [8/10], Step [600/600], Loss: 0.0855\n",
            "Epoch [9/10], Step [100/600], Loss: 0.1737\n",
            "Epoch [9/10], Step [200/600], Loss: 0.1844\n",
            "Epoch [9/10], Step [300/600], Loss: 0.0800\n",
            "Epoch [9/10], Step [400/600], Loss: 0.0222\n",
            "Epoch [9/10], Step [500/600], Loss: 0.0272\n",
            "Epoch [9/10], Step [600/600], Loss: 0.2209\n",
            "Epoch [10/10], Step [100/600], Loss: 0.0346\n",
            "Epoch [10/10], Step [200/600], Loss: 0.0746\n",
            "Epoch [10/10], Step [300/600], Loss: 0.0458\n",
            "Epoch [10/10], Step [400/600], Loss: 0.1939\n",
            "Epoch [10/10], Step [500/600], Loss: 0.1264\n",
            "Epoch [10/10], Step [600/600], Loss: 0.1439\n",
            "Accuracy of the RNN(\n",
            "  (rnn): RNN(28, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
            ") on the 10000 test images: 97.73 % \n",
            "\n",
            "Start training GRU(\n",
            "  (gru): GRU(28, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Epoch [1/10], Step [100/600], Loss: 0.9000\n",
            "Epoch [1/10], Step [200/600], Loss: 0.3917\n",
            "Epoch [1/10], Step [300/600], Loss: 0.3162\n",
            "Epoch [1/10], Step [400/600], Loss: 0.1644\n",
            "Epoch [1/10], Step [500/600], Loss: 0.2775\n",
            "Epoch [1/10], Step [600/600], Loss: 0.3238\n",
            "Epoch [2/10], Step [100/600], Loss: 0.1486\n",
            "Epoch [2/10], Step [200/600], Loss: 0.1284\n",
            "Epoch [2/10], Step [300/600], Loss: 0.1863\n",
            "Epoch [2/10], Step [400/600], Loss: 0.0740\n",
            "Epoch [2/10], Step [500/600], Loss: 0.0918\n",
            "Epoch [2/10], Step [600/600], Loss: 0.1127\n",
            "Epoch [3/10], Step [100/600], Loss: 0.0343\n",
            "Epoch [3/10], Step [200/600], Loss: 0.1012\n",
            "Epoch [3/10], Step [300/600], Loss: 0.0491\n",
            "Epoch [3/10], Step [400/600], Loss: 0.1258\n",
            "Epoch [3/10], Step [500/600], Loss: 0.1186\n",
            "Epoch [3/10], Step [600/600], Loss: 0.0188\n",
            "Epoch [4/10], Step [100/600], Loss: 0.0652\n",
            "Epoch [4/10], Step [200/600], Loss: 0.0298\n",
            "Epoch [4/10], Step [300/600], Loss: 0.0343\n",
            "Epoch [4/10], Step [400/600], Loss: 0.0153\n",
            "Epoch [4/10], Step [500/600], Loss: 0.0379\n",
            "Epoch [4/10], Step [600/600], Loss: 0.0110\n",
            "Epoch [5/10], Step [100/600], Loss: 0.0290\n",
            "Epoch [5/10], Step [200/600], Loss: 0.0187\n",
            "Epoch [5/10], Step [300/600], Loss: 0.0201\n",
            "Epoch [5/10], Step [400/600], Loss: 0.0455\n",
            "Epoch [5/10], Step [500/600], Loss: 0.0049\n",
            "Epoch [5/10], Step [600/600], Loss: 0.0637\n",
            "Epoch [6/10], Step [100/600], Loss: 0.0640\n",
            "Epoch [6/10], Step [200/600], Loss: 0.0056\n",
            "Epoch [6/10], Step [300/600], Loss: 0.0242\n",
            "Epoch [6/10], Step [400/600], Loss: 0.1239\n",
            "Epoch [6/10], Step [500/600], Loss: 0.0063\n",
            "Epoch [6/10], Step [600/600], Loss: 0.0217\n",
            "Epoch [7/10], Step [100/600], Loss: 0.0030\n",
            "Epoch [7/10], Step [200/600], Loss: 0.0346\n",
            "Epoch [7/10], Step [300/600], Loss: 0.0032\n",
            "Epoch [7/10], Step [400/600], Loss: 0.0028\n",
            "Epoch [7/10], Step [500/600], Loss: 0.0066\n",
            "Epoch [7/10], Step [600/600], Loss: 0.0444\n",
            "Epoch [8/10], Step [100/600], Loss: 0.0033\n",
            "Epoch [8/10], Step [200/600], Loss: 0.0586\n",
            "Epoch [8/10], Step [300/600], Loss: 0.0258\n",
            "Epoch [8/10], Step [400/600], Loss: 0.0057\n",
            "Epoch [8/10], Step [500/600], Loss: 0.0595\n",
            "Epoch [8/10], Step [600/600], Loss: 0.0212\n",
            "Epoch [9/10], Step [100/600], Loss: 0.0274\n",
            "Epoch [9/10], Step [200/600], Loss: 0.0036\n",
            "Epoch [9/10], Step [300/600], Loss: 0.0133\n",
            "Epoch [9/10], Step [400/600], Loss: 0.0053\n",
            "Epoch [9/10], Step [500/600], Loss: 0.0052\n",
            "Epoch [9/10], Step [600/600], Loss: 0.0057\n",
            "Epoch [10/10], Step [100/600], Loss: 0.0041\n",
            "Epoch [10/10], Step [200/600], Loss: 0.0036\n",
            "Epoch [10/10], Step [300/600], Loss: 0.0031\n",
            "Epoch [10/10], Step [400/600], Loss: 0.0374\n",
            "Epoch [10/10], Step [500/600], Loss: 0.0054\n",
            "Epoch [10/10], Step [600/600], Loss: 0.0025\n",
            "Accuracy of the GRU(\n",
            "  (gru): GRU(28, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
            ") on the 10000 test images: 98.72 % \n",
            "\n",
            "Start training LSTM(\n",
            "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Epoch [1/10], Step [100/600], Loss: 0.8107\n",
            "Epoch [1/10], Step [200/600], Loss: 0.3344\n",
            "Epoch [1/10], Step [300/600], Loss: 0.2319\n",
            "Epoch [1/10], Step [400/600], Loss: 0.3273\n",
            "Epoch [1/10], Step [500/600], Loss: 0.1287\n",
            "Epoch [1/10], Step [600/600], Loss: 0.0898\n",
            "Epoch [2/10], Step [100/600], Loss: 0.0892\n",
            "Epoch [2/10], Step [200/600], Loss: 0.1625\n",
            "Epoch [2/10], Step [300/600], Loss: 0.1212\n",
            "Epoch [2/10], Step [400/600], Loss: 0.1386\n",
            "Epoch [2/10], Step [500/600], Loss: 0.1825\n",
            "Epoch [2/10], Step [600/600], Loss: 0.0665\n",
            "Epoch [3/10], Step [100/600], Loss: 0.1791\n",
            "Epoch [3/10], Step [200/600], Loss: 0.0176\n",
            "Epoch [3/10], Step [300/600], Loss: 0.0277\n",
            "Epoch [3/10], Step [400/600], Loss: 0.0717\n",
            "Epoch [3/10], Step [500/600], Loss: 0.0692\n",
            "Epoch [3/10], Step [600/600], Loss: 0.0570\n",
            "Epoch [4/10], Step [100/600], Loss: 0.0110\n",
            "Epoch [4/10], Step [200/600], Loss: 0.0428\n",
            "Epoch [4/10], Step [300/600], Loss: 0.0414\n",
            "Epoch [4/10], Step [400/600], Loss: 0.0696\n",
            "Epoch [4/10], Step [500/600], Loss: 0.0924\n",
            "Epoch [4/10], Step [600/600], Loss: 0.1286\n",
            "Epoch [5/10], Step [100/600], Loss: 0.0184\n",
            "Epoch [5/10], Step [200/600], Loss: 0.0267\n",
            "Epoch [5/10], Step [300/600], Loss: 0.1098\n",
            "Epoch [5/10], Step [400/600], Loss: 0.0181\n",
            "Epoch [5/10], Step [500/600], Loss: 0.0912\n",
            "Epoch [5/10], Step [600/600], Loss: 0.0722\n",
            "Epoch [6/10], Step [100/600], Loss: 0.0955\n",
            "Epoch [6/10], Step [200/600], Loss: 0.0070\n",
            "Epoch [6/10], Step [300/600], Loss: 0.0060\n",
            "Epoch [6/10], Step [400/600], Loss: 0.0135\n",
            "Epoch [6/10], Step [500/600], Loss: 0.0454\n",
            "Epoch [6/10], Step [600/600], Loss: 0.0455\n",
            "Epoch [7/10], Step [100/600], Loss: 0.0598\n",
            "Epoch [7/10], Step [200/600], Loss: 0.0101\n",
            "Epoch [7/10], Step [300/600], Loss: 0.0336\n",
            "Epoch [7/10], Step [400/600], Loss: 0.0096\n",
            "Epoch [7/10], Step [500/600], Loss: 0.0719\n",
            "Epoch [7/10], Step [600/600], Loss: 0.0321\n",
            "Epoch [8/10], Step [100/600], Loss: 0.0178\n",
            "Epoch [8/10], Step [200/600], Loss: 0.0064\n",
            "Epoch [8/10], Step [300/600], Loss: 0.0151\n",
            "Epoch [8/10], Step [400/600], Loss: 0.0284\n",
            "Epoch [8/10], Step [500/600], Loss: 0.1254\n",
            "Epoch [8/10], Step [600/600], Loss: 0.0137\n",
            "Epoch [9/10], Step [100/600], Loss: 0.0456\n",
            "Epoch [9/10], Step [200/600], Loss: 0.0410\n",
            "Epoch [9/10], Step [300/600], Loss: 0.1016\n",
            "Epoch [9/10], Step [400/600], Loss: 0.0935\n",
            "Epoch [9/10], Step [500/600], Loss: 0.0190\n",
            "Epoch [9/10], Step [600/600], Loss: 0.0290\n",
            "Epoch [10/10], Step [100/600], Loss: 0.0063\n",
            "Epoch [10/10], Step [200/600], Loss: 0.0367\n",
            "Epoch [10/10], Step [300/600], Loss: 0.0432\n",
            "Epoch [10/10], Step [400/600], Loss: 0.0719\n",
            "Epoch [10/10], Step [500/600], Loss: 0.0089\n",
            "Epoch [10/10], Step [600/600], Loss: 0.0099\n",
            "Accuracy of the LSTM(\n",
            "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
            ") on the 10000 test images: 98.57 % \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model_rnn = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "model_gru = GRU(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "model_lstm = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for model in [model_rnn,model_gru,model_lstm]:\n",
        "  print(f\"Start training {model}\\n\")\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, (images, labels) in enumerate(train_loader):\n",
        "          # origin shape: [N, 1, 28, 28]\n",
        "          # resized: [N, 28, 28]\n",
        "          images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "              print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "  # Test the model\n",
        "  # In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "  # print(f\"Start evaluating {model}\")\n",
        "  with torch.no_grad():\n",
        "      n_correct = 0\n",
        "      n_samples = 0\n",
        "      for images, labels in test_loader:\n",
        "          images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(images)\n",
        "          # max returns (value ,index)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          n_samples += labels.size(0)\n",
        "          n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "      acc = 100.0 * n_correct / n_samples\n",
        "      print(f'Accuracy of the {model} on the 10000 test images: {acc} % \\n')"
      ]
    }
  ]
}